{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10a56905-ed96-4b17-98da-ca824ddfe713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://thebook.io/080228/part05/ch17/01/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d391d630-45aa-41ba-923b-b73757880d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4974b3a0-0e65-4515-8e56-dca11841011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '해보지 않으면 해낼 수 없다'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "599b5f0d-cb3d-4544-8665-9826daf87e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<원문>\n",
      " 해보지 않으면 해낼 수 없다\n",
      "\n",
      "<토큰화>\n",
      " ['해보지', '않으면', '해낼', '수', '없다']\n"
     ]
    }
   ],
   "source": [
    "result = text_to_word_sequence(text)   #  text_to_word_sequence\n",
    "print(\"<원문>\\n\", text)\n",
    "print(\"\\n<토큰화>\\n\", result)  # Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b8b7ff4-28c8-447f-ac86-abe46a103169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이렇게 단어 단위로 짤라 주는게\n",
    "# text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccc42210-16d0-495d-bd40-9dc119742691",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['먼저 텍스트의 각 단어를 나누어 토큰화 합니다.',\n",
    "       '텍스트의 단어로 토큰화 해야 딥러닝에서 인식됩니다.',\n",
    "       '토큰화 한 결과는 딥러닝에서 사용 할 수 있습니다.',\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c9e4ba9-8331-4916-b518-17ecc743f516",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer()\n",
    "token.fit_on_texts(docs)  # fit 이 들어가니까, 나름 얘도 내부적으로 뭔가,  비지도 학습을 하는거임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c782d4c-8e24-490d-a234-86b63ce002b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<단어 카운트>\n",
      " OrderedDict([('먼저', 1), ('텍스트의', 2), ('각', 1), ('단어를', 1), ('나누어', 1), ('토큰화', 3), ('합니다', 1), ('단어로', 1), ('해야', 1), ('딥러닝에서', 2), ('인식됩니다', 1), ('한', 1), ('결과는', 1), ('사용', 1), ('할', 1), ('수', 1), ('있습니다', 1)])\n"
     ]
    }
   ],
   "source": [
    "print(\"<단어 카운트>\\n\", token.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "339b1b33-c045-482a-b550-e2eba6aad3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<문장 카운트>\n",
      " 3\n"
     ]
    }
   ],
   "source": [
    "print(\"<문장 카운트>\\n\", token.document_count)   # 문장이 3개 들어왔다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7b6629a-40da-44a3-ac6e-8cf9fafa60c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<각 단어가 몇개의 문자에 포함되어 있는가>\n",
      " defaultdict(<class 'int'>, {'먼저': 1, '토큰화': 3, '텍스트의': 2, '합니다': 1, '각': 1, '단어를': 1, '나누어': 1, '해야': 1, '딥러닝에서': 2, '단어로': 1, '인식됩니다': 1, '한': 1, '있습니다': 1, '수': 1, '사용': 1, '할': 1, '결과는': 1})\n"
     ]
    }
   ],
   "source": [
    "print(\"<각 단어가 몇개의 문자에 포함되어 있는가>\\n\", token.word_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62fadb7c-e25f-48b1-9448-8f15437cf900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 문장에 다 들어있는 단어는 관사(a, the 등)일 확률이 높다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c090160a-b1da-4ab2-b64f-eb60e2dc286e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<각 단어에 매겨진 인덱스 값>\n",
      " {'토큰화': 1, '텍스트의': 2, '딥러닝에서': 3, '먼저': 4, '각': 5, '단어를': 6, '나누어': 7, '합니다': 8, '단어로': 9, '해야': 10, '인식됩니다': 11, '한': 12, '결과는': 13, '사용': 14, '할': 15, '수': 16, '있습니다': 17}\n"
     ]
    }
   ],
   "source": [
    "print(\"<각 단어에 매겨진 인덱스 값>\\n\", token.word_index)   #  token이 각 단어에 index를 부여함.  아래와 같이.\n",
    "# 인덱스가 0부터가 아닌 1부터 시작.\n",
    "# 자연어의 로직을 좀 편하게 구현하려다 보니까 이렇게 됨.\n",
    "# 자연어 처리쪽은 1부터 씀\n",
    "\n",
    "# 0부터 쓸 순 있는데, 그럼 여러가지 불편해짐. -> 그래서 1부터 씀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17d852cb-2e60-4598-ac43-c06f80b7c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원-핫 인코딩(one-hot encoding)\n",
    "# 단어는 6개가 있는데,\n",
    "# (0인덱스) 오랫동안 꿈꾸는 이는 그 꿈을 닮아간다\n",
    "#[   0        0       0     0    0   0     0   ]\n",
    "# 출력은 7개가 됨.\n",
    "# 0번째는 안쓰기때문."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17be2d7a-4fcd-41a0-918e-1f9e71274a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'오랫동안': 1, '꿈꾸는': 2, '이는': 3, '그': 4, '꿈을': 5, '닮아간다': 6}\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 함수를 불러와 단어 단위로 토큰화하고 각 단어의 인덱스 값을 출력해 봅니다.\n",
    "text='오랫동안 꿈꾸는 이는 그 꿈을 닮아간다'\n",
    "\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts([text])\n",
    "print(token.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f7202a5-3408-4a1a-ac26-f2731babdb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "x = token.texts_to_sequences([text])   # index만 갖고 옴\n",
    "print(x)    # [[1, 2, 3, 4, 5, 6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a4d685c-644b-47bb-9525-5699707ab38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import to_categorical\n",
    "\n",
    "# 인덱스 수에 하나를 추가해서 원-핫 인코딩 배열 만들기\n",
    "#word_size = len(t.word_index) +1\n",
    "word_size = len(token.word_index) +1    # 웟-핫 인코딩 -> + 1 만큼 배열을 줌  (0번째 때문)\n",
    "\n",
    "# x = to_categorical(x, num_classes=word_size)\n",
    "x = tf.keras.utils.to_categorical(x, num_classes=word_size)\n",
    "# class의 사이즈는 항상 +1 해줘야 함 -> 1부터밖에 안생김\n",
    "# num_classes = word_size = len(token.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "109166bb-8a43-4ca1-8ef7-b73d63311d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914417c-c290-4d35-a0a2-5bf8adb397a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fd52a1e-b324-41d5-9d7d-b2a9e7821947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index로 바꾼 단어의 사이즈가 들어옴\n",
    "\n",
    "# input_length는 옵션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98fc0a9c-822d-4e9c-9e17-6b33900ea979",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"너무 재밌네요\",\"최고예요\",\"참 잘 만든 영화예요\",\"추천하고 싶은 영화입니다\",\\\n",
    "        \"한번 더 보고싶네요\",\"글쎄요\",\"별로예요\",\"생각보다 지루하네요\",\"연기가 어색해요\",\"재미없어요\"]\n",
    "\n",
    "# 앞 5개는 긍정, 뒤 5개는 부정적 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8d665bc-79bc-4bce-a982-fadc5da0d7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0 , 0])   # 긍정은 1, 부정은 0으로 정답을 미리 만듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b1014de-d07e-4339-950b-7a0223f17fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'너무': 1, '재밌네요': 2, '최고예요': 3, '참': 4, '잘': 5, '만든': 6, '영화예요': 7, '추천하고': 8, '싶은': 9, '영화입니다': 10, '한번': 11, '더': 12, '보고싶네요': 13, '글쎄요': 14, '별로예요': 15, '생각보다': 16, '지루하네요': 17, '연기가': 18, '어색해요': 19, '재미없어요': 20}\n"
     ]
    }
   ],
   "source": [
    "# 저 문장을 tokenizer 합시다\n",
    "\n",
    "token = Tokenizer()   # tokenizer 객체 만들고\n",
    "token.fit_on_texts(docs)   # 도커나이즈함\n",
    "print(token.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81242d2e-02d0-4798-b99c-8835c5e88649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰 텍스트 토큰화 결과\n",
      " [[1, 2], [3], [4, 5, 6, 7], [8, 9, 10], [11, 12, 13], [14], [15], [16, 17], [18, 19], [20]]\n"
     ]
    }
   ],
   "source": [
    "# 이 상태에서 index 뽑아냄  .  list로.\n",
    "\n",
    "x = token.texts_to_sequences(docs)\n",
    "print(\"리뷰 텍스트 토큰화 결과\\n\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f35a0c31-8a77-4075-a48c-a200a8a8ccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10개의 입력 feature들이 있음.  첫번째는 1,2  , 두번째는 3 ....\n",
    "# 입력feature의 길이는 고정되어있음. 버라이어티하지 않음.\n",
    "# 그래야 가중치 생성하고 기타 등등 함,\n",
    "# 얘는 입력 feature가 들쑥날쑥해서, 학습이 힘들\n",
    "# 그래서 고정비로 맞춰야 함\n",
    "# 많아야 4개.  -> 4개로 맞출 순 있음.\n",
    "# 그러나 일반적으론 그렇지 않음. 훨씬 길 수도 있음.\n",
    "# 가장 긴놈을 기준으로 할 수도 있지만, 적당히 짜르기도 함.\n",
    "\n",
    "# 한글같은 경우는 동사가 제일 뒤에 나옴.\n",
    "# 주어는 앞에 나옴\n",
    "\n",
    "# 영어 같은 경우는\n",
    "# 앞쪽에 주어, 동사 다 나옴. 앞쪽에 중요한거 다 나옴.  그래서 뒤쪽을 자르기 쉬움.(크게 부담이 없음) -> 그래서 자르기도 함\n",
    "\n",
    "#어쨋든 잘라야 함.\n",
    "# 짜를 때, 맞춰주는 기능이 있음. -> 바로 이거 -> 패팅 => padded_x = pad_sequences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a63c8df-bf2f-4e85-8313-745af5807e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "패딩 결과\n",
      " [[ 0  0  1  2]\n",
      " [ 0  0  0  3]\n",
      " [ 4  5  6  7]\n",
      " [ 0  8  9 10]\n",
      " [ 0 11 12 13]\n",
      " [ 0  0  0 14]\n",
      " [ 0  0  0 15]\n",
      " [ 0  0 16 17]\n",
      " [ 0  0 18 19]\n",
      " [ 0  0  0 20]]\n"
     ]
    }
   ],
   "source": [
    "padded_x = pad_sequences(x, 4)   #  4보다 긴건 짜르라는 소리.   x는 입력값\n",
    "print(\"패딩 결과\\n\", padded_x)\n",
    "\n",
    "# 길이가 4로 맞춰짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "302413de-91db-429a-99a8-9e74a27b662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아까 임베딩쪽을 보면 단어 길이가 보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4b31cb1-9a22-4ed0-b71a-13688d82396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding() 함수는 최소 2개의 매개변수를 필요로 하는데, 바로 ‘입력’과 ‘출력’의 크기입니다. \n",
    "# 위 예제에서 Embedding(16,4)가 의미하는 바는 \n",
    "# 입력될 총 단어 수는 16, 임베딩 후 출력되는 벡터 크기는 4로 하겠다는 뜻입니다. \n",
    "# 여기에 단어를 매번 얼마나 입력할지를 추가로 지정할 수 있습니다. \n",
    "# Embedding(16,4, input_length=2)라고 하면 총 입력되는 단어 수는 16개이지만 매번 2개씩만 넣겠다는 뜻입니다.\n",
    "\n",
    "# 단어 임베딩의 예는 다음 절에서 직접 실습하면서 확인해 보겠습니다. 단어 임베딩을 포함하며 지금까지 배운 내용을 모두 적용해 텍스트 감정을 예측하는 딥러닝 모델을 만들어 보겠습니다.\n",
    "\n",
    "#from keras.layers import Embedding\n",
    "\n",
    "#model = Sequential()\n",
    "#model.add(Embedding(16,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "367e24a3-1df4-4957-a9f1-4e0053500c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_size = len(token.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92c87b77-3922-40e3-8a9d-15b51a5a9890",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(word_size, 8, input_length=4))  # 얘를 8차원 벡터로 만들겠다.\n",
    "model.add(Flatten())  # 이렇게 나온 출력을 Flatten으로 펼치겠다\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# 만든 시퀀스 오브젝트 model에 5개의 노드를 Dense레이어를 통해 연결해줍니다. \n",
    "# 여기서 add를 통해 하나의 레이어를 추가해주는 것입니다.\n",
    "# Dense 레이어는 입력과 출력을 모두 연결해주며 입력과 출력을 각각 연결해주는 가중치를 포함하고 있습니다. \n",
    "# 입력이 3개 출력이 4개라면 가중치는 총 3X4인 12개가 존재하게 됩니다. \n",
    "# Dense레이어는 머신러닝의 기본층으로 영상이나 서로 연속적으로 상관관계가 있는 데이터가 아니라면 \n",
    "# Dense레이어를 통해 학습시킬 수 있는 데이터가 많다는 뜻이 됩니다. \n",
    "\n",
    "# Dense의 첫번째 인자 : 출력 뉴런(노드)의 수를 결정\n",
    "# Dense의 두번째 인자 : input_dim은 입력 뉴런(노드)의 수를 결정, 맨 처음 입력층에서만 사용\n",
    "# Dense의 세번째 인자 : activation 활성화 함수를 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ef75f15-4ae0-4ddf-9275-34cf408466fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 4, 8)              168       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 201\n",
      "Trainable params: 201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "\n",
    "# 32개가 쏙아져 들어오면, dense 1 까지 33개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c6c4b9f-c1a9-4351-8d5e-30f30e42e3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 1s 567ms/step - loss: 0.6912 - accuracy: 0.6000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6890 - accuracy: 0.7000\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6868 - accuracy: 0.7000\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6846 - accuracy: 0.7000\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6824 - accuracy: 0.7000\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6802 - accuracy: 0.7000\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6780 - accuracy: 0.7000\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6759 - accuracy: 0.7000\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6737 - accuracy: 0.8000\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6715 - accuracy: 0.8000\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6693 - accuracy: 0.8000\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6670 - accuracy: 0.8000\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6648 - accuracy: 0.8000\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6626 - accuracy: 0.8000\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6604 - accuracy: 0.8000\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6582 - accuracy: 0.9000\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6559 - accuracy: 0.9000\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6537 - accuracy: 0.9000\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6514 - accuracy: 0.9000\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6492 - accuracy: 0.9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2914991fc70>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(padded_x, classes, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1ec760c-7213-4c6b-8e7e-6a3516a6acee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 147ms/step - loss: 0.6469 - accuracy: 0.9000\n",
      "\n",
      " Accuracy: 0.9000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Accuracy: %.4f\" % (model.evaluate(padded_x, classes)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9cc3b1-c825-494f-ad0c-69f947f521ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
