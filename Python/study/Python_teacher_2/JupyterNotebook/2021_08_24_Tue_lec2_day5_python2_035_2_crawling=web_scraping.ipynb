{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d1da15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "\n",
    "url = 'www.naver.com'\n",
    "webbrowser.open(url)  #  이러면 chrome(인터넷창)으로 www.naver.com에 접속된 창이 뜸."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939aba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# r = request.get('https://www.google.co.kr')  # response [200]은 정상이라는 의미\n",
    "# print(r)  # >>> <Response [200]>\n",
    "\n",
    "# print(type(r))   #  >>>  <class 'requests.models.Response'>\n",
    "# print(r.text)\n",
    "# >>>\n",
    "# <!doctype html><html itemscope=\"\" itemtype=\"http://schema.org/WebPage\" lang\n",
    "# =\"ko\"><head><meta content=\"text/html; charset=UT........"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94ac421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install bs4\n",
    "# conda install lxml   # lxml -> 파서 -> 구문 분석기  -> 소스를 구문분석할 것\n",
    "# 2개 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0f1e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cc282fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup을 이용해 구문분석(파싱)을 해보자\n",
    "html = \"\"\"<html><body><div><span>\n",
    "       <a href=http://www.naver.com>naver</a>\n",
    "       <a href=https://www.google.com>google</a>\n",
    "       <a href=http://www.daum.net/>daum</a>\n",
    "       </span></div></body></html>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d23ed9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><body><div><span>\n",
      "<a href=\"http://www.naver.com\">naver</a>\n",
      "<a href=\"https://www.google.com\">google</a>\n",
      "<a href=\"http://www.daum.net/\">daum</a>\n",
      "</span></div></body></html>\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html, 'lxml')\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a75ea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <body>\n",
      "  <div>\n",
      "   <span>\n",
      "    <a href=\"http://www.naver.com\">\n",
      "     naver\n",
      "    </a>\n",
      "    <a href=\"https://www.google.com\">\n",
      "     google\n",
      "    </a>\n",
      "    <a href=\"http://www.daum.net/\">\n",
      "     daum\n",
      "    </a>\n",
      "   </span>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify())   #  이해하기 쉽게 계층적으로 표시해줌\n",
    "\n",
    "# 아래에서 <a ~~> -> Enter 태그 </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a17099d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"http://www.naver.com\">naver</a>\n"
     ]
    }
   ],
   "source": [
    "print(soup.find('a'))  # Enter태그 (<a >로 시작) 제일 앞에거 하나 뽑아주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "766fc113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver\n"
     ]
    }
   ],
   "source": [
    "print(soup.find('a').get_text())  # 안에 있는 text 뽑아줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e67ccba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"http://www.naver.com\">naver</a>, <a href=\"https://www.google.com\">google</a>, <a href=\"http://www.daum.net/\">daum</a>]\n"
     ]
    }
   ],
   "source": [
    "# Enter태그 3개를 다 가져오려면\n",
    "site_names = soup.find_all('a')\n",
    "print(site_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e70a954b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver\n",
      "google\n",
      "daum\n"
     ]
    }
   ],
   "source": [
    "# site names가 list로 되어있음. -> 다 뽑으려면 for문으로 삥글삥글 돌리면 됨\n",
    "for site_name in site_names:\n",
    "    print(site_name.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "277fd546",
   "metadata": {},
   "outputs": [],
   "source": [
    "html2 = \"\"\"\n",
    "<html>\n",
    " <head>\n",
    " <title>작품과 작가 모음</title>\n",
    " </head>\n",
    " <body>\n",
    " <h1>책 정보</h1>\n",
    " <p id=\"book_title\">토지</p>\n",
    " <p id=\"author\">박경리</p>\n",
    " <p id=\"book_title\">태백산맥</p>\n",
    " <p id=\"author\">조정래</p>\n",
    " <p id=\"book_title\">감옥으로부터의 사색</p>\n",
    " <p id=\"author\">신영복</p>\n",
    " </body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cec073f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2 = BeautifulSoup(html2, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73c7b950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>작품과 작가 모음</title>\n"
     ]
    }
   ],
   "source": [
    "# title만 가져오기\n",
    "print(soup2.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f359884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<body>\n",
      "<h1>책 정보</h1>\n",
      "<p id=\"book_title\">토지</p>\n",
      "<p id=\"author\">박경리</p>\n",
      "<p id=\"book_title\">태백산맥</p>\n",
      "<p id=\"author\">조정래</p>\n",
      "<p id=\"book_title\">감옥으로부터의 사색</p>\n",
      "<p id=\"author\">신영복</p>\n",
      "</body>\n"
     ]
    }
   ],
   "source": [
    "print(soup2.body)   # body 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "955aa4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>책 정보</h1>\n"
     ]
    }
   ],
   "source": [
    "print(soup2.body.h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8dd25a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p id=\"book_title\">토지</p>\n"
     ]
    }
   ],
   "source": [
    "# id가 book_title인 놈을 찾아보자\n",
    "print(soup2.find('p', {'id':'book_title'}))  # soup2 = BeautifulSoup(html2, 'lxml')   /  p 인데 id가 book_title인 놈만\n",
    "# 이러면 맨 처음게 걸려 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01d0b355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p id=\"author\">박경리</p>\n"
     ]
    }
   ],
   "source": [
    "print(soup2.find('p', {'id':'author'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fd28ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p id=\"book_title\">토지</p>, <p id=\"book_title\">태백산맥</p>, <p id=\"book_title\">감옥으로부터의 사색</p>]\n",
      "[<p id=\"author\">박경리</p>, <p id=\"author\">조정래</p>, <p id=\"author\">신영복</p>]\n",
      "\n",
      "토지/박경리\n",
      "태백산맥/조정래\n",
      "감옥으로부터의 사색/신영복\n"
     ]
    }
   ],
   "source": [
    "#from bs4 import BeautifulSoup\n",
    "\n",
    "book_titles = soup2.find_all('p', {'id':'book_title'})  # soup2 = BeautifulSoup(html2, 'lxml')   /  p 인데 id가 book_title인 놈만\n",
    "authors = soup2.find_all('p', {'id':'author'})\n",
    "\n",
    "print(book_titles)\n",
    "print(authors)\n",
    "print()\n",
    "\n",
    "for book_title, author in zip(book_titles, authors):\n",
    "    print(book_title.get_text() + '/' + author.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4970de89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>책 정보</h1>\n"
     ]
    }
   ],
   "source": [
    "# 요즘은 css 이놈을 더 많이 씀\n",
    "print(soup2.body.h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "513a9e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h1>책 정보</h1>]\n"
     ]
    }
   ],
   "source": [
    "print(soup2.select('body h1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a7ecd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p id=\"book_title\">토지</p>, <p id=\"author\">박경리</p>, <p id=\"book_title\">태백산맥</p>, <p id=\"author\">조정래</p>, <p id=\"book_title\">감옥으로부터의 사색</p>, <p id=\"author\">신영복</p>]\n"
     ]
    }
   ],
   "source": [
    "print(soup2.select('body p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bdefe82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p id=\"book_title\">토지</p>, <p id=\"book_title\">태백산맥</p>, <p id=\"book_title\">감옥으로부터의 사색</p>]\n"
     ]
    }
   ],
   "source": [
    "print(soup2.select('p#book_title'))  # 이러면 book_title에 있는놈 다 가져오는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "882c4e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/example.html', encoding='utf-8')\n",
    "html3 = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d07c7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html>\n",
      "<html>\n",
      "<head>\n",
      "<meta charset=\"utf-8\">\n",
      "<title>사이트 모음</title>\n",
      "</head>\n",
      "<body>\n",
      "<p id=\"title\">\n",
      "<b>자주 가는 사이트 모음</b>\n",
      "</p>\n",
      "<p id=\"contents\">이곳은 자주 가는 사이트를 모아둔 곳입니다.</p>\n",
      "<a href=\"http://www.naver.com\" class=\"portal\" id=\"naver\">네이버</a> <br>\n",
      "<a href=\"https://www.google.com\" class=\"search\" id=\"google\">구글</a> <br>\n",
      "<a href=\"http://www.daum.net\" class=\"portal\" id=\"daum\">다음</a> <br>\n",
      "<a href=\"http://www.nl.go.kr\" class=\"government\" id=\"nl\">국립중앙도서관</a>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(html3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "873fb793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a class=\"portal\" href=\"http://www.naver.com\" id=\"naver\">네이버</a>, <a class=\"portal\" href=\"http://www.daum.net\" id=\"daum\">다음</a>]\n"
     ]
    }
   ],
   "source": [
    "soup3 = BeautifulSoup(html3, 'lxml')\n",
    "print(soup3.select('a.portal'))  # id=# (샾) ,  class = .(점)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa1ab5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 찾고자 하는 부분 찾을 때\n",
    "# 1) 찾고자 하는 부분을 우클릭 -> 검사(Ctrl+Shift_i) -> 우측에 정보 나옴\n",
    "# 2) F11(우클릭, 검사) 누르면 우측에 나온거중 좌측상단의 마우스모양 누르고, 원하는 부분 왼클릭하면 오른쪽에 해당하는 부분이 표시됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "063ce1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  https://www.alexa.com/topsites/countries/KR  ->  아마존에서 운영하는, 실시간 나라별 접속량이 가장 많은 사이트를 보여줌.\n",
    "# 지금은 한국, 1위부터 구글, 네이버, 유튜브, 다음....\n",
    "# 여기서 Top 랭킹 50개만 가져와보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3a2dba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # -> 이게 있어야 아래의 requests.get(url).text 가 사용 가능\n",
    "\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# 일단 주소를 복사해서 접속을 애햐 함.\n",
    "url = 'https://www.alexa.com/topsites/countries/KR'\n",
    "html_website_ranking = requests.get(url).text  # text 뽑아옴.\n",
    "# 뽑아온걸 바로 BeautifulSoup을 통과시키자\n",
    "soup_website_ranking = BeautifulSoup(html_website_ranking, 'lxml')\n",
    "\n",
    "# 구글 '우클릭, 검색'으로 클릭해보니 p 밑에 a임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a320267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"https://support.alexa.com/hc/en-us/articles/200444340\" target=\"_blank\">this explanation</a>, <a href=\"/siteinfo/google.com\">Google.com</a>, <a href=\"/siteinfo/naver.com\">Naver.com</a>, <a href=\"/siteinfo/youtube.com\">Youtube.com</a>, <a href=\"/siteinfo/daum.net\">Daum.net</a>, <a href=\"/siteinfo/tistory.com\">Tistory.com</a>]\n"
     ]
    }
   ],
   "source": [
    "website_ranking = soup_website_ranking.select('p a')\n",
    "print(website_ranking[:6])  # slice해서 5개만 찍어봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39f8b4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "website_ranking = website_ranking[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8092ce4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(website_ranking))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8b9e211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이트 명만 list로 만들어 볼것.\n",
    "website_ranking_sites = [element.get_text()   # wesite에서 뽑아와서 하나씩 .get_text() 하면 됨\n",
    "                        for element in \\\n",
    "                        website_ranking] \n",
    "\n",
    "# text를 바로는 못가져옴. soup으로 가져와서 처리를 해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e0fc937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Google.com', 'Naver.com', 'Youtube.com', 'Daum.net', 'Tistory.com']\n"
     ]
    }
   ],
   "source": [
    "print(website_ranking_sites[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "02894b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Top Sites in South Korea]\n",
      "1: Google.com\n",
      "2: Naver.com\n",
      "3: Youtube.com\n",
      "4: Daum.net\n",
      "5: Tistory.com\n",
      "6: Kakao.com\n",
      "7: Tmall.com\n",
      "8: Coupang.com\n",
      "9: Google.co.kr\n",
      "10: Amazon.com\n",
      "11: Netflix.com\n",
      "12: Sohu.com\n",
      "13: Facebook.com\n",
      "14: Namu.wiki\n",
      "15: Qq.com\n",
      "16: Wikipedia.org\n",
      "17: Taobao.com\n",
      "18: Jd.com\n",
      "19: 360.cn\n",
      "20: Baidu.com\n",
      "21: Yahoo.com\n",
      "22: Microsoft.com\n",
      "23: Adobe.com\n",
      "24: Dcinside.com\n",
      "25: Instagram.com\n",
      "26: Twitch.tv\n",
      "27: Zoom.us\n",
      "28: Donga.com\n",
      "29: Chosun.com\n",
      "30: Nate.com\n",
      "31: Office.com\n",
      "32: Sina.com.cn\n",
      "33: Fmkorea.com\n",
      "34: 11st.co.kr\n",
      "35: Stackoverflow.com\n",
      "36: Saramin.co.kr\n",
      "37: Weibo.com\n",
      "38: Jobkorea.co.kr\n",
      "39: Ebay.com\n",
      "40: Danawa.com\n",
      "41: Gmarket.co.kr\n",
      "42: Apple.com\n",
      "43: Aliexpress.com\n",
      "44: Afreecatv.com\n",
      "45: Yna.co.kr\n",
      "46: Cafe24.com\n",
      "47: Bing.com\n",
      "48: Nicovideo.jp\n",
      "49: Yahoo.co.jp\n",
      "50: Inven.co.kr\n"
     ]
    }
   ],
   "source": [
    "print('[Top Sites in South Korea]')\n",
    "for k in range(len(website_ranking)):\n",
    "    print(\"{0}: {1}\".format(k+1, website_ranking_sites[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ac7b315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ranking_dict = {'Website': website_ranking_sites}\n",
    "df = pd.DataFrame(ranking_dict, \\\n",
    "                  index=range(1, len(website_ranking)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d834db44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Website\n",
      "1          Google.com\n",
      "2           Naver.com\n",
      "3         Youtube.com\n",
      "4            Daum.net\n",
      "5         Tistory.com\n",
      "6           Kakao.com\n",
      "7           Tmall.com\n",
      "8         Coupang.com\n",
      "9        Google.co.kr\n",
      "10         Amazon.com\n",
      "11        Netflix.com\n",
      "12           Sohu.com\n",
      "13       Facebook.com\n",
      "14          Namu.wiki\n",
      "15             Qq.com\n",
      "16      Wikipedia.org\n",
      "17         Taobao.com\n",
      "18             Jd.com\n",
      "19             360.cn\n",
      "20          Baidu.com\n",
      "21          Yahoo.com\n",
      "22      Microsoft.com\n",
      "23          Adobe.com\n",
      "24       Dcinside.com\n",
      "25      Instagram.com\n",
      "26          Twitch.tv\n",
      "27            Zoom.us\n",
      "28          Donga.com\n",
      "29         Chosun.com\n",
      "30           Nate.com\n",
      "31         Office.com\n",
      "32        Sina.com.cn\n",
      "33        Fmkorea.com\n",
      "34         11st.co.kr\n",
      "35  Stackoverflow.com\n",
      "36      Saramin.co.kr\n",
      "37          Weibo.com\n",
      "38     Jobkorea.co.kr\n",
      "39           Ebay.com\n",
      "40         Danawa.com\n",
      "41      Gmarket.co.kr\n",
      "42          Apple.com\n",
      "43     Aliexpress.com\n",
      "44      Afreecatv.com\n",
      "45          Yna.co.kr\n",
      "46         Cafe24.com\n",
      "47           Bing.com\n",
      "48       Nicovideo.jp\n",
      "49        Yahoo.co.jp\n",
      "50        Inven.co.kr\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8c05661d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "# 이번엔 image 끌어내려보기\n",
    "# 그런데 image를 다운받으려면 test를 먼저 해봐야 함.  ->  image를 down이 가능한 곳이 있고, 없는 곳이 있음.\n",
    "# 1) 이미지 주소 복사\n",
    "url = 'https://www.python.org/static/img/python-logo.png'\n",
    "html_image = requests.get(url)\n",
    "print(html_image)  # <Response [200]>   ->  리스폰스 200이 떨어지면 다운 받을 수 있다는 소리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "817df4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python-logo.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_name = os.path.basename(url)   # 전체 url 주소에서 맨 끝에 있는 부분을 짤라내는 기능  -> 이러면 맨 뒤에놈을 짤래내줌\n",
    "print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2d479c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.python.org/static/img/python-logo.png'\n",
    "html_image = requests.get(url)\n",
    "file_name = os.path.basename(url)\n",
    "folder = 'download'\n",
    "\n",
    "if not os.path.exists(folder):   #  download 폴더가 존재하지 않으면\n",
    "    os.makedirs(folder)       #  만들어라  /  있으면 패스\n",
    "    \n",
    "image_path = os.path.join(folder, file_name)   #  download 밑에 / 뭐~  이렇게 붙여주는 것  => image_path = download\\python-logo.png\n",
    "\n",
    "imageFile = open(image_path, 'wb')  # 저장할 거니까 'w', -> 이건 binary 코드 파일(text가 아님 -> text로 읽으려고 하면 안됨)-> 그래서'wb'\n",
    "\n",
    "# 부담을 줄이려고 버퍼링(일부 받았다가 한번에 쓰고, 일부 받아서 한번에 쓰고 하는 것)쓰는데, 이 양을 조절할 수 있음.\n",
    "chunk_size = 1000000\n",
    "for chunk in html_image.iter_content(chunk_size):\n",
    "    imageFile.write(chunk)  # imageFile에다가 write를 해라 (chunk) <- 얘를\n",
    "    \n",
    "imageFile.close()\n",
    "\n",
    "# 이러면 이 JupyterNotebook 파일이 있는 폴더에 download폴더가 생기고 그 안에 image가 다운받아져 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f207613e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download\\python-logo.png\n"
     ]
    }
   ],
   "source": [
    "# 이러면 위의 join의 역할을 눈으로 볼 수 있음  ->  붙여주는 것\n",
    "print(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "880cad69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<img alt=\"Download Masked Crab on a Rocky Shore FREE Stock Photo\" class=\"image\" data-src=\"https://picjumbo.com/wp-content/uploads/masked-crab-on-a-rocky-shore-free-photo-1080x586.jpg\" itemprop=\"contentUrl\" src=\"https://i0.wp.com/picjumbo.com/wp-content/uploads/masked-crab-on-a-rocky-shore-free-photo.jpg?w=5&amp;quality=20&amp;strip=all\" title=\"Download Masked Crab on a Rocky Shore FREE Stock Photo\" width=\"1080\"/>, <img alt=\"Download Little Crab on The Rocky Shore FREE Stock Photo\" class=\"image\" data-src=\"https://picjumbo.com/wp-content/uploads/little-crab-on-the-rocky-shore-free-photo-1080x720.jpg\" itemprop=\"contentUrl\" src=\"https://i0.wp.com/picjumbo.com/wp-content/uploads/little-crab-on-the-rocky-shore-free-photo.jpg?w=5&amp;quality=20&amp;strip=all\" title=\"Download Little Crab on The Rocky Shore FREE Stock Photo\" width=\"1080\"/>, <img alt=\"Download Walking Cute Yorkshire Terrier FREE Stock Photo\" class=\"image\" data-src=\"https://picjumbo.com/wp-content/uploads/walking-cute-yorkshire-terrier-free-photo-1080x640.jpg\" itemprop=\"contentUrl\" src=\"https://i0.wp.com/picjumbo.com/wp-content/uploads/walking-cute-yorkshire-terrier-free-photo.jpg?w=5&amp;quality=20&amp;strip=all\" title=\"Download Walking Cute Yorkshire Terrier FREE Stock Photo\" width=\"1080\"/>, <img alt=\"Download Perfectly Masked Crab on a Rock FREE Stock Photo\" class=\"image\" data-src=\"https://picjumbo.com/wp-content/uploads/perfectly-masked-crab-on-a-rock-free-photo-1080x720.jpg\" itemprop=\"contentUrl\" src=\"https://i0.wp.com/picjumbo.com/wp-content/uploads/perfectly-masked-crab-on-a-rock-free-photo.jpg?w=5&amp;quality=20&amp;strip=all\" title=\"Download Perfectly Masked Crab on a Rock FREE Stock Photo\" width=\"1080\"/>, <img alt=\"Download Husky Dog on a Frozen Lake FREE Stock Photo\" class=\"image\" data-src=\"https://picjumbo.com/wp-content/uploads/husky-dog-on-a-frozen-lake-free-photo-1080x1620.jpg\" itemprop=\"contentUrl\" src=\"https://i0.wp.com/picjumbo.com/wp-content/uploads/husky-dog-on-a-frozen-lake-free-photo.jpg?w=5&amp;quality=20&amp;strip=all\" title=\"Download Husky Dog on a Frozen Lake FREE Stock Photo\" width=\"1080\"/>]\n"
     ]
    }
   ],
   "source": [
    "# https://picjumbo.com/  -> 다운받는건 무료 / 그러나 blog 등에 사용하는건 유료일 수 있음. 알아봐야 함.\n",
    "# https://picjumbo.com/free-stock-photos/animals/   ->  이 페이지에 있는 그림을 싸그리 긁어와 봅시다.\n",
    "# 1) 일단 주소 복사\n",
    "url = 'https://picjumbo.com/free-stock-photos/animals/'\n",
    "html_picjumbo = requests.get(url).text\n",
    "soup_picjumbo = BeautifulSoup(html_picjumbo, 'lxml')\n",
    "image_elements = soup_picjumbo.select('picture img')\n",
    "print(image_elements[:5])  # data-src (데이타 소스) 밑에 이미지 주소가 있다 -> 이걸 뽑아내야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c2425572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://picjumbo.com/wp-content/uploads/masked-crab-on-a-rocky-shore-free-photo-1080x586.jpg\n"
     ]
    }
   ],
   "source": [
    "image_url = image_elements[0].get('data-src')\n",
    "print(image_url)  # 확인 -> 제대로 갖고 옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a2a5745a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_url(url):\n",
    "    html_image_url = requests.get(url).text\n",
    "    soup_image_url = BeautifulSoup(html_image_url, 'lxml')\n",
    "    image_elements = soup_image_url.select('picture img')\n",
    "    if image_elements != None:  # 존재하면\n",
    "        image_urls = []\n",
    "        for image_element in image_elements:\n",
    "            image_urls.append(image_element.get('data-src'))\n",
    "            \n",
    "        return image_urls\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# 위에서 했던거 그대로 함수로 만든 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5bd22243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 폴더 지정해서 저장하는 함수 만들기\n",
    "\n",
    "def download_image(img_folder, img_url):\n",
    "    if img_url != None:\n",
    "        html_image = requests.get(img_url)\n",
    "        image_file = open(os.path.join(img_folder, \\\n",
    "                                      os.path.basename(img_url)), 'wb')  # 위에서 몇단계에 했던걸 한방에 한것.  새로한건 없음\n",
    "        chunk_size = 1000000  # -> 이게 없으면 계속 접속해야하는데, 부담. 그래서 일정량 모아서 한번에 쓰는 것\n",
    "        for chunk in html_image.iter_content(chunk_size):\n",
    "            image_file.write(chunk)\n",
    "        \n",
    "        # 빙글빙글 끝났으면\n",
    "        image_file.close()\n",
    "#        print('이미지 파일:{0} 받기 완료'.format(\\\n",
    "#                                        os.path.basename(img_url))) # 실제 이미지 파일만 print -> 이미지파일  ~~ 받기 완료  # -> 출력문\n",
    "    else:\n",
    "        print(\"이미지가 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8466a181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 파일:masked-crab-on-a-rocky-shore-free-photo-1080x586.jpg 받기 완료\n",
      "이미지 파일:little-crab-on-the-rocky-shore-free-photo-1080x720.jpg 받기 완료\n",
      "이미지 파일:walking-cute-yorkshire-terrier-free-photo-1080x640.jpg 받기 완료\n",
      "22\n",
      "==================================================\n",
      "이미지 받기 완료!!!\n"
     ]
    }
   ],
   "source": [
    "url = 'https://picjumbo.com/free-stock-photos/animals/'\n",
    "figure_folder = 'download'\n",
    "picjumbo_image_urls = get_image_url(url)  # list로 return 해주니 list로 받는 것\n",
    "# 일단 test 삼아서 3개만 받아오기\n",
    "for k in range(3):\n",
    "    download_image(figure_folder, picjumbo_image_urls[k])\n",
    "    \n",
    "# print(len(picjumbo_image_urls))  -> 22\n",
    "    \n",
    "print('=' * 50)\n",
    "print(\"이미지 받기 완료!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aa04cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install tqdm  ->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d4f5100b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2126880916494ebe0ca8519ad5b2aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "이미지 받기 완료!!!\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "url = 'https://picjumbo.com/free-stock-photos/animals/'\n",
    "figure_folder = 'download'\n",
    "picjumbo_image_urls = get_image_url(url)  # list로 return 해주니 list로 받는 것\n",
    "# 일단 test 삼아서 3개만 받아오기\n",
    "for k in tqdm(range(len(picjumbo_image_urls))):\n",
    "    download_image(figure_folder, picjumbo_image_urls[k])  # -> 위에서 주소 print한거 주석(#)으로 막고\n",
    "                                                            # -> len(picjumbo_image_urls) 전체 leng으로 돌리고\n",
    "                                                    # 전체를 tqdm으로 묶은 다음 실행하면 -> 현재 진행 상황이 그래프로 나타남.\n",
    "    \n",
    "# print(len(picjumbo_image_urls))  -> 22\n",
    "    \n",
    "print('=' * 50)\n",
    "print(\"이미지 받기 완료!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51acb856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 내일부턴 단순히 데이터를 긁어오는게 아니고\n",
    "# 조금이라도 분석 해보자."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
